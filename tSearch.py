# This program requires Twitter API key stored in tAuth.cfg file, generated by ConfigParser
# Author: KimJ2
from TwitterSearch import *
import psycopg2
import codecs
import json
import time
import sys
import ConfigParser
import datetime

def dbRun():
    geo_config = {}
    geo_config['dc'] = '38.89439,-77.034675'
    geo_config['moscow'] = '55.652798,37.573242'
    geo_config['ukraine'] = '49.353756,32.695313'

    search_range = '500.0'

    output = codecs.open('ukraine_russia.csv', 'w', 'utf-8')
    #output.write("tid,from_user,from_user_name,from_user_id,created_at,iso_language_code,text,geo_type,geo_lon,geo_lat\n")
    #output.close()

    data_log = codecs.open('data_log.log', 'a', 'utf-8')
    error_log = codecs.open('error_log.log', 'a', 'utf-8')

    total = 0

    try:
        while True:

            try:
                conn = psycopg2.connect("dbname='twitter' user='postgres' host='localhost' password='PASSWORD'")
            except:
                error_log.write("I am unable to connect to the database\n")
                #sys.exit(1)

            cur = conn.cursor()
# Future JSON display option placed 
            # TODO: json_data needed be defined
            json_data = json_data.replace('\/', '/') #remove escapes on URLs

            try:
                json_object = json.loads(json_data)
            except Exception, e:
                error_log.write("\n\nERROR: "+unicode(e)+"\n\n")
                continue

            results = json_object['results']
            geo_count = 0

            for result in json_object['results']:
                if result['geo']:
                    tid             = result['id']
                    from_user       = result['from_user'].replace('\'', '\\\'').replace('\"', '\\\"')
                    from_user_name      = result['from_user_name'].replace('\'', '\\\'').replace('\"', '\\\"')
                    link_to_user        = 'http://twitter.com/'+from_user
                    from_user_id        = result['from_user_id']
                    created_at      = result['created_at'].replace(',', '')
                    iso_language_code   = result['iso_language_code']
                    text            = result['text'].replace('\'', '\\\'').replace('\"', '\\\"')
                    geo_type        = result['geo']['type']
                    geo_lon         = result['geo']['coordinates'][1]
                    geo_lat         = result['geo']['coordinates'][0]
            
                    date_split = created_at.split()
                    month = get_month_number(date_split[2])
                    zone = date_split[5][3:]
                    sql_date = date_split[3]+"-"+unicode(month)+"-"+date_split[1]+" "+date_split[4]+"+"+unicode(zone)

                    if geo_lon != 0.0 and geo_lat != 0.0:
                        sql = "INSERT INTO twitter_data(id, from_user, from_user_name, from_user_id, created_at, iso_language_code, text, geo_lon, geo_lat, geom, link_to_user) VALUES ("+unicode(tid)+", E\'"+from_user+"\', E\'"+from_user_name+"\', "+unicode(from_user_id)+", \'"+sql_date+"\', \'"+iso_language_code+"\', E\'"+text+"\', "+unicode(geo_lon)+", "+unicode(geo_lat)+", ST_GeomFromText('POINT("+unicode(geo_lon)+" "+unicode(geo_lat)+")', 4326), E\'"+unicode(link_to_user)+"\');"
                        #print sql

                        try:                    
                            cur.execute(sql)
                        except Exception, e:
                            error_log.write("EXCEPTION:   "+unicode(e)+"\n")
                            error_log.write("SLQ:   "+sql+"\n")
                            continue

                        geo_count += 1
                        total += 1

                data_log.write(city+" - "+unicode(geo_count)+" / "+unicode(total)+"\n")
                time.sleep(1)

            time.sleep(15)

    except Exception, e:
        error_log.write("\n\nERROR: "+unicode(e)+"\n\n")

def insertTwDb(tweet_all): # Connects to DB and insert tweets string into each field
    try:
        conn = psycopg2.connect("dbname='twitter' user='postgres' host='localhost' password='PASSWORD'")
    except:
        error_log.write("I am unable to connect to the database\n")
        sys.exit(1)

    cur = conn.cursor()
    # SQL to insert 
    #sql = "INSERT INTO table_name(field1, field2, field3) VALUES ('1,2,3');"
    field_names = "city`lat`lon`contributors`truncated`text`in_reply_to_status_id`id`favorite_count`retweeted`coordinates`source`in_reply_to_screen_name`in_reply_to_user_id`retweet_count`id_str`favorited`user`geo`in_reply_to_user_id_str`lang`created_at`in_reply_to_status_id_str`place`metadata"
    vlaues = tweet_all.split('`')
    sql = "INSERT INTO tweets(" + field_names + ") VALUES (" + tweet_all + ");" 
    try:
        cur.execute(sql)
    except Exception, e:
        error_log.write("EXCEPTION: "+unicode(e)+"\n")
        error_log.write("SQL: "+sql+"\n")
############
def run():
    geo_config = {}
    geo_config['dc'] = '38.89439,-77.034675'
    geo_config['moscow'] = '55.75,37.5833333333'
    geo_config['ukraine'] = '50.4333333333,30.5166666667'
    radius = 800
    tweet_count = 0
    geo_count = 0
    output = codecs.open('ukraine_russia.csv', 'w', 'utf-8')
    error_log = codecs.open('error_log.log', 'a', 'utf-8')
    output.write ("city`lat`lon`")
    #output.write("t_id\tt_lang\tt_place\tt_rt_cnt\tt_source\tt_geo\tt_coord\tt_created_at\tt_urls\tt_text\tt_m_url\n")

    # Get Twitter API key Auth info
    config = ConfigParser.RawConfigParser()
    config.read('tAuth1.cfg')
    consumer_key = config.get('TwitterApiKey', 'consumer_key')

    # Connect DB
    try:
        conn = psycopg2.connect("dbname='sns' user='postgres' host='localhost' password='algus4fkd'")
    except:
        error_log.write("I am unable to connect to the database\n")
        sys.exit(1)

    cur = conn.cursor()
    try:
        #for city in geo_config: city has string of two float numbers - lat and lon
        for city in geo_config: 
            tso = TwitterSearchOrder() # create a TwitterSearchOrder object
            tso.setKeywords(['Ukraine', 'Russia']) # let's define all words we would like to have a look for
            #tso.setLanguage('en') # we want to see English tweets only
            tso.setCount(7) # only give us 7 results per page
            tso.setIncludeEntities(False) # and don't give us all those entity information
            city_geo = geo_config[city].split(',')
            city_lat = city_geo[0]
            city_lon = city_geo[1]
            print (city + ': ' + city_lat + ' / ' + city_lon)
            tso.setGeocode(float(city_lat),float(city_lon),radius,km=True) #setGeocode(latitude<float>, longitude<float>, radius<int,long>, km=<True,False>
            #tso.setUntil(datetime.date(2014,07,21))
            #tso.setCount(100)
            # it's about time to create a TwitterSearch object with our secret tokens
            ts = TwitterSearch(
                consumer_key = config.get('TwitterApiKey', 'consumer_key'),
                consumer_secret = config.get('TwitterApiKey', 'consumer_secret'),
                access_token= config.get('TwitterApiKey', 'access_token'),
                access_token_secret= config.get('TwitterApiKey', 'access_token_secret')
            )
            for tweet in ts.searchTweetsIterable(tso): # this is where the fun actually starts :)
                attr_count = 0
                contributors = unicode(tweet['contributors']).replace(',', ';').replace('\'','')
                truncated = unicode(tweet['truncated']).replace(',', ';').replace('\'','').replace('#','HT:')
                text = unicode(tweet['text']).replace(',', ';').replace('\'','').replace('#','HT:')
                in_reply_to_status_id = unicode(tweet['in_reply_to_status_id']).replace(',', ';').replace('\'','')
                t_id = unicode(tweet['id']).replace(',', ';').replace('\'','')
                favorite_count = unicode(tweet['favorite_count']).replace(',', ';').replace('\'','')
                retweeted = unicode(tweet['retweeted']).replace(',', ';').replace('\'','')
                coordinates = unicode(tweet['coordinates']).replace(',', ';').replace('\'','')
                source = unicode(tweet['source']).replace(',', ';').replace('\'','')
                in_reply_to_screen_name = unicode(tweet['in_reply_to_screen_name']).replace(',', ';').replace('\'','').replace('#','HT:')
                retweet_count = unicode(tweet['retweet_count']).replace(',', ';').replace('\'','')
                id_str = unicode(tweet['id_str']).replace(',', ';').replace('\'','')
                favorited = unicode(tweet['favorited']).replace(',', ';').replace('\'','')
                user_name = unicode(tweet['user']).replace(',', ';').replace('\'','').replace('#','HT:')
                geo = unicode(tweet['geo']).replace(',', ';').replace('\'','')
                lang = unicode(tweet['lang']).replace(',', ';').replace('\'','')
                created_at = unicode(tweet['created_at']).replace(',', ';').replace('\'','')
                place = unicode(tweet['place']).replace(',', ';').replace('\'','').replace('#','HT:')
                metadata = unicode(tweet['metadata']).replace(',', ';').replace('\'','').replace('#','HT:')
                if tweet['geo']:
                    lat = unicode(tweet['geo']['coordinates'][0]).replace(',', ';').replace('\'','')
                    lon = unicode(tweet['geo']['coordinates'][1]).replace(',', ';').replace('\'','')
                    #
                    # DB attributes
                    # Ingest
                    geo_count += 1
                    row = city + '`' + unicode(lat) + '`' + unicode(lon)
                    field_names = "city, lat, lon, contributors, truncated, text, t_id, favorite_count, retweeted, coordinates, source, retweet_count, id_str, user_name, geo, lang, created_at, place, geom, metadata"
                    tweet_all = '\'' + city + '\',\'' + lat + '\',\'' + lon + '\',\'' + contributors + '\',\'' + truncated + '\',\'' + text + '\',\'' + t_id + '\',\'' + favorite_count + '\',\'' + retweeted + '\',\'' + coordinates + '\',\'' + source + '\',\'' + retweet_count + '\',\'' + id_str + '\',\'' + user_name + '\',\'' + geo + '\',\'' + lang + '\',\'' + created_at + '\',\'' + place + '\',ST_SetSRID(ST_MakePoint(' + lon + ', ' + lat + '), 4326),\'' + metadata + '\''
                else:
                    row = city + '`' + '`'
                    field_names = "city, contributors, truncated, text, t_id, favorite_count, retweeted, coordinates, source, retweet_count, id_str, user_name, geo, lang, created_at, place, geom, metadata"
                    tweet_all = '\'' + city + '\',\'' + contributors + '\',\'' + truncated + '\',\'' + text + '\',\'' + t_id + '\',\'' + favorite_count + '\',\'' + retweeted + '\',\'' + coordinates + '\',\'' + source + '\',\'' + retweet_count + '\',\'' + id_str + '\',\'' + user_name + '\',\'' + geo + '\',\'' + lang + '\',\'' + created_at + '\',\'' + place + '\',ST_SetSRID(ST_MakePoint(' + city_lon + ', ' + city_lat + '), 4326),\'' + metadata + '\''
                # Write to a text file
                # All tweets whether they have geocodes
                sql = "INSERT INTO tweets (" + field_names + ") SELECT " + tweet_all + " WHERE NOT EXISTS (SELECT t_id FROM tweets WHERE t_id='" + t_id + "');" 
                #sql = "IF NOT EXISTS (SELECT * FROM tweets WHERE t_id = '" + t_id + "') INSERT INTO tweets (" + field_names + ") VALUES (" + tweet_all + ");"
                try:
                    cur.execute(sql)
                    conn.commit()
                except Exception, e:
                    error_log.write("EXCEPTION: "+unicode(e)+"\n")
                    error_log.write("SQL: "+sql+"\n")
                
                for attr in tweet:
                    attr_str = unicode(tweet[attr])
                    #attr_str = attr_str.replace(',', ';')
                    row = row + '`' + attr_str
                    attr_count += 1
                    if tweet_count == 0:
                        output.write (attr + '`')
                output.write ('\n'+row)
                #t_m_url = tweet['entities']['media']['url']
                #print( '%s,%s' % ( tweet['user']['screen_name'], tweet['text'] ) )
                # Print the tweets to a file
                #row = str(t_id) + '\t' + unicode(t_lang) + '\t' + unicode(t_place) + '\t' + str(t_rt_cnt) + '\t' + unicode(t_source) + '\t' + unicode(t_geo) + '\t' + unicode(t_coord) + '\t' + unicode(t_created_at) + '\t' + unicode(t_text) + '\n'

                #insertTwDb(tweet)
                tweet_count += 1
                time.sleep(1)
            output.close()
            #conn.close()
            time.sleep(15)
            print ("Total Tweets: %d / Total Geo-ed Tweets: %d" % (tweet_count, geo_count))
    except TwitterSearchException as e: # take care of all those ugly errors if there are some
        print(e)  
    #config.close() 
    conn.close()

if __name__ == "__main__":
    run()
